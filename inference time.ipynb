{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "691683d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for VGG16 model:\n",
      "Avg Inference Time over 100 frames: 273.94 ms\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from torchvision import transforms, models \n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import wave\n",
    "\n",
    "# Transformations for the input image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "class CustomVGG16(nn.Module):\n",
    "    def __init__(self, num_classes=6, dropout_rate=0.7): \n",
    "        super(CustomVGG16, self).__init__()\n",
    "\n",
    "        model = models.vgg16(pretrained=True)\n",
    "        for param in model.features[8:].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            *list(model.features.children())[:24], \n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            *list(model.features.children())[24:], \n",
    "        )\n",
    "        self.avgpool = model.avgpool\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Load the VGG model\n",
    "model = CustomVGG16(num_classes=6) \n",
    "model.load_state_dict(torch.load('best_model_checkpoint.pth', map_location='cpu'))  \n",
    "model.to('cpu') \n",
    "model.eval()  \n",
    "\n",
    "def classify_frames_using_vgg(num_frames_to_process=100):   # Process 100 frames as an example\n",
    "    cap = cv2.VideoCapture(1)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Unable to open video feed.\")\n",
    "        return\n",
    "\n",
    "    total_inference_time = 0.0\n",
    "    num_frames = 0\n",
    "\n",
    "    while num_frames < num_frames_to_process:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        pil_image = Image.fromarray(frame)\n",
    "        transformed_image = transform(pil_image)\n",
    "        batch_t = torch.unsqueeze(transformed_image, 0).to('cpu')\n",
    "\n",
    "        # Measure inference time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            out = model(batch_t)\n",
    "\n",
    "        end_time = time.time()\n",
    "        inference_time = (end_time - start_time) * 1000  \n",
    "\n",
    "        \n",
    "        total_inference_time += inference_time\n",
    "        num_frames += 1\n",
    "\n",
    "    average_inference_time = total_inference_time / num_frames\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    print(\"for VGG16 model:\")\n",
    "    print(f\"Avg Inference Time over {num_frames} frames: {average_inference_time:.2f} ms\")\n",
    "\n",
    "# Start the frame processing\n",
    "classify_frames_using_vgg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08fa5efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/waqar/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/waqar/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN Inference Time: 0.27 ms\n",
      "Predicted Label: 0.0\n",
      "Commercial detected. Starting frame classification...\n",
      "The commercial belongs to the category: Technology_Electronics_commercial\n",
      "The commercial belongs to the category: Technology_Electronics_commercial\n",
      "The commercial belongs to the category: Technology_Electronics_commercial\n",
      "Average VGG Inference Time: 275.98 ms\n",
      "RNN Inference Time: 0.27 ms\n",
      "Predicted Label: 0.0\n",
      "Commercial detected. Starting frame classification...\n",
      "The commercial belongs to the category: Technology_Electronics_commercial\n",
      "The commercial belongs to the category: Technology_Electronics_commercial\n",
      "The commercial belongs to the category: Technology_Electronics_commercial\n",
      "Average VGG Inference Time: 282.51 ms\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 289\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage VGG Inference Time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_inference_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 289\u001b[0m     \u001b[43mrecord_and_classify_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 231\u001b[0m, in \u001b[0;36mrecord_and_classify_audio\u001b[0;34m()\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError in feature extraction!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# Slide the audio window for next classification\u001b[39;00m\n\u001b[0;32m--> 231\u001b[0m audio_frames \u001b[38;5;241m=\u001b[39m \u001b[43mslide_and_record_additional_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_frames\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m3\u001b[39m)\n",
      "Cell \u001b[0;32mIn[20], line 160\u001b[0m, in \u001b[0;36mslide_and_record_additional_audio\u001b[0;34m(existing_frames, seconds_to_record)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# Record new audio to append\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, frames_to_discard):\n\u001b[0;32m--> 160\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m     existing_frames\u001b[38;5;241m.\u001b[39mappend(data)\n\u001b[1;32m    163\u001b[0m stream\u001b[38;5;241m.\u001b[39mstop_stream()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyaudio/__init__.py:570\u001b[0m, in \u001b[0;36mPyAudio.Stream.read\u001b[0;34m(self, num_frames, exception_on_overflow)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_input:\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot input stream\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    569\u001b[0m                   paCanNotReadFromAnOutputOnlyStream)\n\u001b[0;32m--> 570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mexception_on_overflow\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#final one\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision import transforms, models \n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import wave\n",
    "import pyaudio\n",
    "import time\n",
    "from collections import Counter\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "class CustomVGG16(nn.Module):\n",
    "    def __init__(self, num_classes=6, dropout_rate=0.7): \n",
    "        super(CustomVGG16, self).__init__()\n",
    "\n",
    "        model = models.vgg16(pretrained=True)\n",
    "        for param in model.features[8:].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            *list(model.features.children())[:24], \n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            *list(model.features.children())[24:], \n",
    "        )\n",
    "        self.avgpool = model.avgpool\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "model = CustomVGG16(num_classes=6) \n",
    "model.load_state_dict(torch.load('best_model_checkpoint.pth', map_location='cpu'))  \n",
    "model.to('cpu') \n",
    "model.eval()  \n",
    "\n",
    "class_names = ['Automotive_commercial', 'Entertainment_commercial',\n",
    "               'Food_commercial', 'Healthcare_commercial', 'Insurance_commercial', \n",
    "               'Technology_Electronics_commercial']\n",
    "\n",
    "\n",
    "\n",
    "features_df = pd.read_csv('features.csv')\n",
    "\n",
    "# Extract features and corresponding labels\n",
    "X = features_df.iloc[:, :-1].values  # features \n",
    "y = features_df.iloc[:, -1].values  # labels \n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode the labels\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.int64)\n",
    "\n",
    "# Split the dataset into training, validation, and testing sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define the RNN model\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 64 \n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "model2 = RNN(input_size, hidden_size, num_classes)\n",
    "\n",
    "def extract_features(file_name):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast')\n",
    "\n",
    "        # MFCC (Mel-frequency cepstral coefficients)\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "        mfccs_processed = np.mean(mfccs.T, axis=0)\n",
    "\n",
    "        # Chroma feature\n",
    "        chroma_stft = np.mean(librosa.feature.chroma_stft(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Spectral contrast\n",
    "        spectral_contrast = np.mean(librosa.feature.spectral_contrast(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Spectral centroid\n",
    "        spectral_centroids = np.mean(librosa.feature.spectral_centroid(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Zero-crossing rate\n",
    "        zero_crossing_rate = np.mean(librosa.feature.zero_crossing_rate(y=audio).T, axis=0)\n",
    "\n",
    "        # Spectral rolloff\n",
    "        spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Combine all features into a 1D array\n",
    "        features = np.hstack([mfccs_processed, chroma_stft, spectral_contrast, spectral_centroids, zero_crossing_rate, spectral_rolloff])\n",
    "\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"Error encountered while parsing file: {file_name}\")\n",
    "        return None\n",
    "\n",
    "def slide_and_record_additional_audio(existing_frames, seconds_to_record=10):\n",
    "    sample_rate = 44100\n",
    "    frames_to_discard = int(sample_rate / 1024 * seconds_to_record)\n",
    "\n",
    "    # Discard the first 'seconds_to_record' worth of frames\n",
    "    existing_frames = existing_frames[frames_to_discard:]\n",
    "\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format=pyaudio.paInt16, channels=1, rate=sample_rate, input=True, frames_per_buffer=1024)\n",
    "\n",
    "    # Record new audio to append\n",
    "    for _ in range(0, frames_to_discard):\n",
    "        data = stream.read(1024)\n",
    "        existing_frames.append(data)\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "    return existing_frames  # Return the updated frames\n",
    "\n",
    "def record_and_classify_audio():\n",
    "    audio_duration = 60  # seconds\n",
    "    sample_rate = 44100\n",
    "    audio_frames = []\n",
    "\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format=pyaudio.paInt16, channels=1, rate=sample_rate, input=True, frames_per_buffer=1024)\n",
    "\n",
    "    # Start initial recording\n",
    "    for _ in range(0, int(sample_rate / 1024 * audio_duration)):\n",
    "        data = stream.read(1024)\n",
    "        audio_frames.append(data)\n",
    "\n",
    "    # Main loop for continuous prediction and handling commercials\n",
    "    while True:\n",
    "        # Save the recorded audio to a WAV file\n",
    "        output_audio_file = \"recorded_audio.wav\"\n",
    "        with wave.open(output_audio_file, 'wb') as wf:\n",
    "            wf.setnchannels(1)\n",
    "            wf.setsampwidth(p.get_sample_size(pyaudio.paInt16))\n",
    "            wf.setframerate(sample_rate)\n",
    "            wf.writeframes(b''.join(audio_frames))\n",
    "\n",
    "        # Extract features from audio file\n",
    "        recorded_features = extract_features(output_audio_file)\n",
    "        if recorded_features is not None:\n",
    "            recorded_df = pd.DataFrame([recorded_features])\n",
    "            recorded_df = scaler.transform(recorded_df)\n",
    "\n",
    "            model2.eval()\n",
    "            recorded_tensor = torch.tensor(recorded_df, dtype=torch.float32)\n",
    "\n",
    "    # Start the timer for RNN\n",
    "            start_time_rnn = time.time()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                predicted_output = model2(recorded_tensor.unsqueeze(1))\n",
    "\n",
    "            # End the timer and calculate inference time for RNN\n",
    "            end_time_rnn = time.time()\n",
    "            rnn_inference_time = (end_time_rnn - start_time_rnn) * 1000  # Convert to milliseconds\n",
    "            print(f\"RNN Inference Time: {rnn_inference_time:.2f} ms\")\n",
    "\n",
    "            _, predicted_class = torch.max(predicted_output, 1)\n",
    "            predicted_class_label = label_encoder.inverse_transform(predicted_class.numpy())\n",
    "            print(f\"Predicted Label: {predicted_class_label[0]}\")\n",
    "\n",
    "\n",
    "            # Handle the case where a commercial is detected\n",
    "            if predicted_class_label[0] == 0.0:\n",
    "                print(\"Commercial detected. Starting frame classification...\")\n",
    "                classify_frames_using_vgg()\n",
    "\n",
    "            # Handle the case where a non-commercial is detected\n",
    "            elif predicted_class_label[0] == 1.0:\n",
    "                print(\"End of commercial detected. Stopping frame classification...\")\n",
    "                # You can add any code here to handle the end of a commercial\n",
    "                classify_frames_using_vgg()\n",
    "        else:\n",
    "            print(\"Error in feature extraction!\")\n",
    "\n",
    "        # Slide the audio window for next classification\n",
    "        audio_frames = slide_and_record_additional_audio(audio_frames)\n",
    "\n",
    "        time.sleep(3)  # Sleep before next cycle\n",
    "\n",
    "def classify_frames_using_vgg():\n",
    "    vgg_inference_times = []  # Initialize the list to store inference times\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Unable to open video feed.\")\n",
    "        return\n",
    "\n",
    "    labels_buffer = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        orig_frame = frame.copy()\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        pil_image = Image.fromarray(frame)\n",
    "        transformed_image = transform(pil_image)\n",
    "        batch_t = torch.unsqueeze(transformed_image, 0).to('cpu')\n",
    "\n",
    "        inference_start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            out = model(batch_t)\n",
    "        inference_end_time = time.time()\n",
    "\n",
    "        inference_time = (inference_end_time - inference_start_time) * 1000  # Convert to milliseconds\n",
    "        vgg_inference_times.append(inference_time)\n",
    "\n",
    "        _, predicted = torch.max(out, 1)\n",
    "        label = class_names[predicted.item()]\n",
    "        labels_buffer.append(label)\n",
    "\n",
    "        if len(labels_buffer) == 10:\n",
    "            most_common_label = Counter(labels_buffer).most_common(1)[0][0]\n",
    "            print(f\"The commercial belongs to the category: {most_common_label}\")\n",
    "            labels_buffer.clear()\n",
    "\n",
    "        orig_frame = cv2.cvtColor(orig_frame, cv2.COLOR_RGB2BGR)\n",
    "        cv2.putText(orig_frame, label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "        cv2.imshow('Frame Classification', orig_frame)\n",
    "\n",
    "        # Check if 'q' is pressed or 10 seconds have passed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q') or (time.time() - start_time) >= 10:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Compute and print average VGG inference time\n",
    "    avg_inference_time = sum(vgg_inference_times) / len(vgg_inference_times)\n",
    "    print(f\"Average VGG Inference Time: {avg_inference_time:.2f} ms\")\n",
    "if __name__ == '__main__':\n",
    "    record_and_classify_audio()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c817454c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

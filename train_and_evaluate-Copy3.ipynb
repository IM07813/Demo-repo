{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480a943a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall opencv-python -y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03378deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7bcac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddd38ce",
   "metadata": {},
   "source": [
    "## train the RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbfdfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "features_df = pd.read_csv('features.csv')\n",
    "\n",
    "# Extract features and corresponding labels\n",
    "X = features_df.iloc[:, :-1].values  # features \n",
    "y = features_df.iloc[:, -1].values  # labels \n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode the labels\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.int64)\n",
    "\n",
    "# Split the dataset into training, validation, and testing sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define the RNN model\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 64 \n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "model = RNN(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 300 \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    outputs = model(X_train_tensor.unsqueeze(1))  \n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(outputs.squeeze(), y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {loss.item():.4f}')\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_val_tensor.unsqueeze(1))\n",
    "        val_loss = criterion(val_outputs.squeeze(), y_val)\n",
    "        _, val_predicted = torch.max(val_outputs, 1)\n",
    "        val_accuracy = accuracy_score(y_val.numpy(), val_predicted.numpy())\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss.item():.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test_tensor.unsqueeze(1))\n",
    "    _, test_predicted = torch.max(test_outputs, 1)\n",
    "    test_accuracy = accuracy_score(y_test.numpy(), test_predicted.numpy())\n",
    "\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2689db",
   "metadata": {},
   "source": [
    "## Test phase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32215ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_features(file_name):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast')\n",
    "\n",
    "        # MFCC (Mel-frequency cepstral coefficients)\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "        mfccs_processed = np.mean(mfccs.T, axis=0)\n",
    "\n",
    "        # Chroma feature\n",
    "        chroma_stft = np.mean(librosa.feature.chroma_stft(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Spectral contrast\n",
    "        spectral_contrast = np.mean(librosa.feature.spectral_contrast(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Spectral centroid\n",
    "        spectral_centroids = np.mean(librosa.feature.spectral_centroid(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Zero-crossing rate\n",
    "        zero_crossing_rate = np.mean(librosa.feature.zero_crossing_rate(y=audio).T, axis=0)\n",
    "\n",
    "        # Spectral rolloff\n",
    "        spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Combine all features into a 1D array\n",
    "        features = np.hstack([mfccs_processed, chroma_stft, spectral_contrast, spectral_centroids, zero_crossing_rate, spectral_rolloff])\n",
    "\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"Error encountered while parsing file: {file_name}\")\n",
    "        return None\n",
    "\n",
    "# Specify the directories containing the .mp3 files\n",
    "directories = ['something']\n",
    "\n",
    "# Create an empty DataFrame \n",
    "features_df = pd.DataFrame()\n",
    "\n",
    "for directory in directories:\n",
    "    print(f\"Processing files in {directory} directory\")\n",
    "    for filename in tqdm(os.listdir(directory)):\n",
    "        if filename.endswith('.wav'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            try:\n",
    "                features = extract_features(file_path)\n",
    "                # Append the features to the DataFrame as a new row\n",
    "                if features is not None:\n",
    "                    features_series = pd.Series(features)\n",
    "                    features_df = pd.concat([features_df, features_series], axis=0)  # Concatenate along rows (axis=0)\n",
    "            except Exception as e:\n",
    "                print(f\"Error encountered while processing file: {file_path}\")\n",
    "                continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cfb4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new= features_df.T\n",
    "X_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478e7bf0",
   "metadata": {},
   "source": [
    "## load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7f60ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loaded_model = RNN( input_size, hidden_size, num_classes)\n",
    "\n",
    "\n",
    "loaded_model.load_state_dict(torch.load('rnn_model.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b079b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.eval()  \n",
    "\n",
    "\n",
    "X_new = scaler.transform(X_new) \n",
    "X_new_tensor = torch.tensor(X_new, dtype=torch.float32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    new_outputs = loaded_model(X_new_tensor.unsqueeze(1))\n",
    "    _, new_predicted = torch.max(new_outputs, 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d09666d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173dd34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import numpy as np\n",
    "import librosa\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "def extract_features(file_name):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast')\n",
    "\n",
    "        # MFCC (Mel-frequency cepstral coefficients)\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "        mfccs_processed = np.mean(mfccs.T, axis=0)\n",
    "\n",
    "        # Chroma feature\n",
    "        chroma_stft = np.mean(librosa.feature.chroma_stft(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Spectral contrast\n",
    "        spectral_contrast = np.mean(librosa.feature.spectral_contrast(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Spectral centroid\n",
    "        spectral_centroids = np.mean(librosa.feature.spectral_centroid(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Zero-crossing rate\n",
    "        zero_crossing_rate = np.mean(librosa.feature.zero_crossing_rate(y=audio).T, axis=0)\n",
    "\n",
    "        # Spectral rolloff\n",
    "        spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Combine all features into a 1D array\n",
    "        features = np.hstack([mfccs_processed, chroma_stft, spectral_contrast, spectral_centroids, zero_crossing_rate, spectral_rolloff])\n",
    "\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"Error encountered while parsing file: {file_name}\")\n",
    "        return None\n",
    "\n",
    "# Initialize PyAudio and settings\n",
    "p = pyaudio.PyAudio()\n",
    "sample_rate = 44100\n",
    "channel = 1\n",
    "sample_format = pyaudio.paFloat32\n",
    "frame_length = 1024\n",
    "audio_duration = 2  # 2 seconds\n",
    "buffer_size = int(sample_rate * audio_duration / frame_length)\n",
    "\n",
    "audio_buffer = np.zeros(buffer_size, dtype=np.float32)\n",
    "\n",
    "stream = p.open(format=sample_format,\n",
    "                channels=channel,\n",
    "                rate=sample_rate,\n",
    "                input=True,\n",
    "                frames_per_buffer=frame_length)\n",
    "\n",
    "print(\"Press Ctrl+C to stop...\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        # Shift the buffer\n",
    "        audio_buffer[:-frame_length] = audio_buffer[frame_length:]\n",
    "\n",
    "        # Read new audio frame and append to buffer\n",
    "        audio_frame = np.frombuffer(stream.read(frame_length), dtype=np.float32)\n",
    "        audio_buffer[-frame_length:] = audio_frame\n",
    "\n",
    "        # Feature extraction\n",
    "        features = extract_features(audio_buffer, sample_rate)\n",
    "        \n",
    "        if len(features) == 0:\n",
    "            print(\"Error in feature extraction.\")\n",
    "            continue\n",
    "\n",
    "        # Feature scaling, assuming 'scaler' exists\n",
    "        scaled_features = scaler.transform([features])\n",
    "\n",
    "        # Classification, assuming 'model' and 'label_encoder' exist\n",
    "        model.eval()\n",
    "        tensor_input = torch.tensor(scaled_features, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            predicted_output = model(tensor_input.unsqueeze(1))\n",
    "            _, predicted_class = torch.max(predicted_output, 1)\n",
    "        \n",
    "        predicted_class_label = label_encoder.inverse_transform(predicted_class.numpy())\n",
    "        print(f\"Predicted Class: {predicted_class_label[0]}\")\n",
    "\n",
    "        # You can add sleep here if needed\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Stopped.\")\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        p.terminate()\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b52b17",
   "metadata": {},
   "source": [
    "## Sliding the window by 1 second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5595e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "import time\n",
    "import os\n",
    "import pandas as pd  \n",
    "import torch\n",
    "\n",
    "\n",
    "def extract_features(file_name):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast')\n",
    "\n",
    "        # MFCC (Mel-frequency cepstral coefficients)\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "        mfccs_processed = np.mean(mfccs.T, axis=0)\n",
    "\n",
    "        # Chroma feature\n",
    "        chroma_stft = np.mean(librosa.feature.chroma_stft(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Spectral contrast\n",
    "        spectral_contrast = np.mean(librosa.feature.spectral_contrast(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Spectral centroid\n",
    "        spectral_centroids = np.mean(librosa.feature.spectral_centroid(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Zero-crossing rate\n",
    "        zero_crossing_rate = np.mean(librosa.feature.zero_crossing_rate(y=audio).T, axis=0)\n",
    "\n",
    "        # Spectral rolloff\n",
    "        spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Combine all features into a 1D array\n",
    "        features = np.hstack([mfccs_processed, chroma_stft, spectral_contrast, spectral_centroids, zero_crossing_rate, spectral_rolloff])\n",
    "\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"Error encountered while parsing file: {file_name}\")\n",
    "        return None\n",
    "\n",
    "def record_and_classify_audio():\n",
    "    audio_duration = 60  # Record audio for 1 minute\n",
    "    sample_rate = 44100  # 44.1kHz\n",
    "    ten_sec_frames = int(sample_rate / 1024 * 10)  \n",
    "\n",
    "    audio_frames = []\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    stream = p.open(format=pyaudio.paInt16, channels=1, rate=sample_rate, input=True, frames_per_buffer=1024)\n",
    "    \n",
    "    # Initialize the buffer with 1-minute audio data\n",
    "    print(\"Initializing audio buffer with 1-minute audio...\")\n",
    "    for _ in range(0, int(sample_rate / 1024 * audio_duration)):\n",
    "        data = stream.read(1024)\n",
    "        audio_frames.append(data)\n",
    "    print(\"Initialization complete.\")\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        print(\"Sliding the window by 10 seconds...\")\n",
    "        audio_frames = audio_frames[ten_sec_frames:]\n",
    "        \n",
    "        for _ in range(0, ten_sec_frames):\n",
    "            data = stream.read(1024)\n",
    "            audio_frames.append(data)\n",
    "\n",
    "        \n",
    "        output_audio_file = \"recorded_audio.wav\"\n",
    "        wf = wave.open(output_audio_file, 'wb')\n",
    "        wf.setnchannels(1)\n",
    "        wf.setsampwidth(p.get_sample_size(pyaudio.paInt16))\n",
    "        wf.setframerate(sample_rate)\n",
    "        wf.writeframes(b''.join(audio_frames))\n",
    "        wf.close()\n",
    "\n",
    "        # extract features \n",
    "        recorded_features = extract_features(output_audio_file)\n",
    "\n",
    "        if recorded_features is not None:\n",
    "            recorded_df = pd.DataFrame([recorded_features])\n",
    "            recorded_df = scaler.transform(recorded_df)\n",
    "            model.eval()\n",
    "            recorded_tensor = torch.tensor(recorded_df, dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                predicted_output = model(recorded_tensor.unsqueeze(1))\n",
    "                _, predicted_class = torch.max(predicted_output, 1)\n",
    "\n",
    "            predicted_class_label = label_encoder.inverse_transform(predicted_class.numpy())\n",
    "            print(\"Predicted Class:\", predicted_class_label[0])\n",
    "\n",
    "                \n",
    "\n",
    "            # Delete the audio file\n",
    "            os.remove(output_audio_file)\n",
    "        else:\n",
    "            print(\"Error occurred while extracting features from recorded audio.\")\n",
    "\n",
    "        time.sleep(0.1)\n",
    "\n",
    "# Entry point\n",
    "if __name__ == \"__main__\":\n",
    "    record_and_classify_audio()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4208cd48",
   "metadata": {},
   "source": [
    "## Sliding the window by 0.1 second\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18d7660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "import time\n",
    "import os\n",
    "import pandas as pd  \n",
    "import torch\n",
    "\n",
    "\n",
    "def extract_features(file_name):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast')\n",
    "\n",
    "        # MFCC (Mel-frequency cepstral coefficients)\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "        mfccs_processed = np.mean(mfccs.T, axis=0)\n",
    "\n",
    "        # Chroma feature\n",
    "        chroma_stft = np.mean(librosa.feature.chroma_stft(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Spectral contrast\n",
    "        spectral_contrast = np.mean(librosa.feature.spectral_contrast(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Spectral centroid\n",
    "        spectral_centroids = np.mean(librosa.feature.spectral_centroid(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Zero-crossing rate\n",
    "        zero_crossing_rate = np.mean(librosa.feature.zero_crossing_rate(y=audio).T, axis=0)\n",
    "\n",
    "        # Spectral rolloff\n",
    "        spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Combine all features into a 1D array\n",
    "        features = np.hstack([mfccs_processed, chroma_stft, spectral_contrast, spectral_centroids, zero_crossing_rate, spectral_rolloff])\n",
    "\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"Error encountered while parsing file: {file_name}\")\n",
    "        return None\n",
    "\n",
    "def record_and_classify_audio():\n",
    "    audio_duration = 60  # Record audio for 1 minute\n",
    "    sample_rate = 44100  # 44.1kHz\n",
    "    _frames_ = int(sample_rate / 1024 * 0.1)  \n",
    "\n",
    "    audio_frames = []\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    stream = p.open(format=pyaudio.paInt16, channels=1, rate=sample_rate, input=True, frames_per_buffer=1024)\n",
    "    \n",
    "    # Initialize the buffer with 1-minute audio data\n",
    "    print(\"Initializing audio buffer with 1-minute audio...\")\n",
    "    for _ in range(0, int(sample_rate / 1024 * audio_duration)):\n",
    "        data = stream.read(1024)\n",
    "        audio_frames.append(data)\n",
    "    print(\"Initialization complete.\")\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        print(\"Sliding the window by 0.1 seconds...\")\n",
    "        audio_frames = audio_frames[_frames_:]\n",
    "        \n",
    "        for _ in range(0, _frames_):\n",
    "            data = stream.read(1024)\n",
    "            audio_frames.append(data)\n",
    "\n",
    "        \n",
    "        output_audio_file = \"recorded_audio.wav\"\n",
    "        wf = wave.open(output_audio_file, 'wb')\n",
    "        wf.setnchannels(1)\n",
    "        wf.setsampwidth(p.get_sample_size(pyaudio.paInt16))\n",
    "        wf.setframerate(sample_rate)\n",
    "        wf.writeframes(b''.join(audio_frames))\n",
    "        wf.close()\n",
    "\n",
    "        # extract features \n",
    "        recorded_features = extract_features(output_audio_file)\n",
    "\n",
    "        if recorded_features is not None:\n",
    "            recorded_df = pd.DataFrame([recorded_features])\n",
    "            recorded_df = scaler.transform(recorded_df)\n",
    "            model.eval()\n",
    "            recorded_tensor = torch.tensor(recorded_df, dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                predicted_output = model(recorded_tensor.unsqueeze(1))\n",
    "                _, predicted_class = torch.max(predicted_output, 1)\n",
    "\n",
    "            predicted_class_label = label_encoder.inverse_transform(predicted_class.numpy())\n",
    "            print(\"Predicted Class:\", predicted_class_label[0])\n",
    "\n",
    "            # Delete the audio file\n",
    "            os.remove(output_audio_file)\n",
    "        else:\n",
    "            print(\"Error occurred while extracting features from recorded audio.\")\n",
    "\n",
    "        time.sleep(0.1)\n",
    "\n",
    "# Entry point\n",
    "if __name__ == \"__main__\":\n",
    "    record_and_classify_audio()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec7a6d1",
   "metadata": {},
   "source": [
    "## slide window by 0.2 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c8c9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "import time\n",
    "import os\n",
    "import pandas as pd  \n",
    "import torch\n",
    "\n",
    "\n",
    "def extract_features(file_name):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast')\n",
    "\n",
    "        # MFCC (Mel-frequency cepstral coefficients)\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "        mfccs_processed = np.mean(mfccs.T, axis=0)\n",
    "\n",
    "        # Chroma feature\n",
    "        chroma_stft = np.mean(librosa.feature.chroma_stft(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Spectral contrast\n",
    "        spectral_contrast = np.mean(librosa.feature.spectral_contrast(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Spectral centroid\n",
    "        spectral_centroids = np.mean(librosa.feature.spectral_centroid(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Zero-crossing rate\n",
    "        zero_crossing_rate = np.mean(librosa.feature.zero_crossing_rate(y=audio).T, axis=0)\n",
    "\n",
    "        # Spectral rolloff\n",
    "        spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Combine all features into a 1D array\n",
    "        features = np.hstack([mfccs_processed, chroma_stft, spectral_contrast, spectral_centroids, zero_crossing_rate, spectral_rolloff])\n",
    "\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"Error encountered while parsing file: {file_name}\")\n",
    "        return None\n",
    "\n",
    "def record_and_classify_audio():\n",
    "    audio_duration = 60  # Record audio for 1 minute\n",
    "    sample_rate = 44100  # 44.1kHz\n",
    "    _frames_ = int(sample_rate / 1024 * 0.2)  \n",
    "\n",
    "    audio_frames = []\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    stream = p.open(format=pyaudio.paInt16, channels=1, rate=sample_rate, input=True, frames_per_buffer=1024)\n",
    "    \n",
    "    # Initialize the buffer with 1-minute audio data\n",
    "    print(\"Initializing audio buffer with 1-minute audio...\")\n",
    "    for _ in range(0, int(sample_rate / 1024 * audio_duration)):\n",
    "        data = stream.read(1024)\n",
    "        audio_frames.append(data)\n",
    "    print(\"Initialization complete.\")\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        print(\"Sliding the window by 0.2 seconds...\")\n",
    "        audio_frames = audio_frames[_frames_:]\n",
    "        \n",
    "        for _ in range(0, _frames_):\n",
    "            data = stream.read(1024)\n",
    "            audio_frames.append(data)\n",
    "\n",
    "        \n",
    "        output_audio_file = \"recorded_audio.wav\"\n",
    "        wf = wave.open(output_audio_file, 'wb')\n",
    "        wf.setnchannels(1)\n",
    "        wf.setsampwidth(p.get_sample_size(pyaudio.paInt16))\n",
    "        wf.setframerate(sample_rate)\n",
    "        wf.writeframes(b''.join(audio_frames))\n",
    "        wf.close()\n",
    "\n",
    "        # extract features \n",
    "        recorded_features = extract_features(output_audio_file)\n",
    "\n",
    "        if recorded_features is not None:\n",
    "            recorded_df = pd.DataFrame([recorded_features])\n",
    "            recorded_df = scaler.transform(recorded_df)\n",
    "            model.eval()\n",
    "            recorded_tensor = torch.tensor(recorded_df, dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                predicted_output = model(recorded_tensor.unsqueeze(1))\n",
    "                _, predicted_class = torch.max(predicted_output, 1)\n",
    "\n",
    "            predicted_class_label = label_encoder.inverse_transform(predicted_class.numpy())\n",
    "            print(\"Predicted Class:\", predicted_class_label[0])\n",
    "\n",
    "            # Delete the audio file\n",
    "            os.remove(output_audio_file)\n",
    "        else:\n",
    "            print(\"Error occurred while extracting features from recorded audio.\")\n",
    "\n",
    "        time.sleep(0.1)\n",
    "\n",
    "# Entry point\n",
    "if __name__ == \"__main__\":\n",
    "    record_and_classify_audio()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1311f94",
   "metadata": {},
   "source": [
    "## slide window by 0.3 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df092aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "import time\n",
    "import os\n",
    "import pandas as pd  \n",
    "import torch\n",
    "\n",
    "\n",
    "def extract_features(file_name):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast')\n",
    "\n",
    "        # MFCC (Mel-frequency cepstral coefficients)\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "        mfccs_processed = np.mean(mfccs.T, axis=0)\n",
    "\n",
    "        # Chroma feature\n",
    "        chroma_stft = np.mean(librosa.feature.chroma_stft(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Spectral contrast\n",
    "        spectral_contrast = np.mean(librosa.feature.spectral_contrast(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Spectral centroid\n",
    "        spectral_centroids = np.mean(librosa.feature.spectral_centroid(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Zero-crossing rate\n",
    "        zero_crossing_rate = np.mean(librosa.feature.zero_crossing_rate(y=audio).T, axis=0)\n",
    "\n",
    "        # Spectral rolloff\n",
    "        spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Combine all features into a 1D array\n",
    "        features = np.hstack([mfccs_processed, chroma_stft, spectral_contrast, spectral_centroids, zero_crossing_rate, spectral_rolloff])\n",
    "\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"Error encountered while parsing file: {file_name}\")\n",
    "        return None\n",
    "\n",
    "def record_and_classify_audio():\n",
    "    audio_duration = 60  # Record audio for 1 minute\n",
    "    sample_rate = 44100  # 44.1kHz\n",
    "    _frames_ = int(sample_rate / 1024 * 0.3)  \n",
    "\n",
    "    audio_frames = []\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    stream = p.open(format=pyaudio.paInt16, channels=1, rate=sample_rate, input=True, frames_per_buffer=1024)\n",
    "    \n",
    "    # Initialize the buffer with 1-minute audio data\n",
    "    print(\"Initializing audio buffer with 1-minute audio...\")\n",
    "    for _ in range(0, int(sample_rate / 1024 * audio_duration)):\n",
    "        data = stream.read(1024)\n",
    "        audio_frames.append(data)\n",
    "    print(\"Initialization complete.\")\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        print(\"Sliding the window by 0.3 seconds...\")\n",
    "        audio_frames = audio_frames[_frames_:]\n",
    "        \n",
    "        for _ in range(0, _frames_):\n",
    "            data = stream.read(1024)\n",
    "            audio_frames.append(data)\n",
    "\n",
    "        \n",
    "        output_audio_file = \"recorded_audio.wav\"\n",
    "        wf = wave.open(output_audio_file, 'wb')\n",
    "        wf.setnchannels(1)\n",
    "        wf.setsampwidth(p.get_sample_size(pyaudio.paInt16))\n",
    "        wf.setframerate(sample_rate)\n",
    "        wf.writeframes(b''.join(audio_frames))\n",
    "        wf.close()\n",
    "\n",
    "        # extract features \n",
    "        recorded_features = extract_features(output_audio_file)\n",
    "\n",
    "        if recorded_features is not None:\n",
    "            recorded_df = pd.DataFrame([recorded_features])\n",
    "            recorded_df = scaler.transform(recorded_df)\n",
    "            model.eval()\n",
    "            recorded_tensor = torch.tensor(recorded_df, dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                predicted_output = model(recorded_tensor.unsqueeze(1))\n",
    "                _, predicted_class = torch.max(predicted_output, 1)\n",
    "\n",
    "            predicted_class_label = label_encoder.inverse_transform(predicted_class.numpy())\n",
    "            print(\"Predicted Class:\", predicted_class_label[0])\n",
    "\n",
    "            # Delete the audio file\n",
    "            os.remove(output_audio_file)\n",
    "        else:\n",
    "            print(\"Error occurred while extracting features from recorded audio.\")\n",
    "\n",
    "        time.sleep(0.1)\n",
    "\n",
    "# Entry point\n",
    "if __name__ == \"__main__\":\n",
    "    record_and_classify_audio()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daab057",
   "metadata": {},
   "source": [
    "## slide window by 0.4 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a509b4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "import time\n",
    "import os\n",
    "import pandas as pd  \n",
    "import torch\n",
    "\n",
    "\n",
    "def extract_features(file_name):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast')\n",
    "\n",
    "        # MFCC (Mel-frequency cepstral coefficients)\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "        mfccs_processed = np.mean(mfccs.T, axis=0)\n",
    "\n",
    "        # Chroma feature\n",
    "        chroma_stft = np.mean(librosa.feature.chroma_stft(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Spectral contrast\n",
    "        spectral_contrast = np.mean(librosa.feature.spectral_contrast(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Spectral centroid\n",
    "        spectral_centroids = np.mean(librosa.feature.spectral_centroid(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Zero-crossing rate\n",
    "        zero_crossing_rate = np.mean(librosa.feature.zero_crossing_rate(y=audio).T, axis=0)\n",
    "\n",
    "        # Spectral rolloff\n",
    "        spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Combine all features into a 1D array\n",
    "        features = np.hstack([mfccs_processed, chroma_stft, spectral_contrast, spectral_centroids, zero_crossing_rate, spectral_rolloff])\n",
    "\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"Error encountered while parsing file: {file_name}\")\n",
    "        return None\n",
    "\n",
    "def record_and_classify_audio():\n",
    "    audio_duration = 60  # Record audio for 1 minute\n",
    "    sample_rate = 44100  # 44.1kHz\n",
    "    _frames_ = int(sample_rate / 1024 * 0.4)  \n",
    "\n",
    "    audio_frames = []\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    stream = p.open(format=pyaudio.paInt16, channels=1, rate=sample_rate, input=True, frames_per_buffer=1024)\n",
    "    \n",
    "    # Initialize the buffer with 1-minute audio data\n",
    "    print(\"Initializing audio buffer with 1-minute audio...\")\n",
    "    for _ in range(0, int(sample_rate / 1024 * audio_duration)):\n",
    "        data = stream.read(1024)\n",
    "        audio_frames.append(data)\n",
    "    print(\"Initialization complete.\")\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        print(\"Sliding the window by 0.4 seconds...\")\n",
    "        audio_frames = audio_frames[_frames_:]\n",
    "        \n",
    "        for _ in range(0, _frames_):\n",
    "            data = stream.read(1024)\n",
    "            audio_frames.append(data)\n",
    "\n",
    "        \n",
    "        output_audio_file = \"recorded_audio.wav\"\n",
    "        wf = wave.open(output_audio_file, 'wb')\n",
    "        wf.setnchannels(1)\n",
    "        wf.setsampwidth(p.get_sample_size(pyaudio.paInt16))\n",
    "        wf.setframerate(sample_rate)\n",
    "        wf.writeframes(b''.join(audio_frames))\n",
    "        wf.close()\n",
    "\n",
    "        # extract features \n",
    "        recorded_features = extract_features(output_audio_file)\n",
    "\n",
    "        if recorded_features is not None:\n",
    "            recorded_df = pd.DataFrame([recorded_features])\n",
    "            recorded_df = scaler.transform(recorded_df)\n",
    "            model.eval()\n",
    "            recorded_tensor = torch.tensor(recorded_df, dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                predicted_output = model(recorded_tensor.unsqueeze(1))\n",
    "                _, predicted_class = torch.max(predicted_output, 1)\n",
    "\n",
    "            predicted_class_label = label_encoder.inverse_transform(predicted_class.numpy())\n",
    "            print(\"Predicted Class:\", predicted_class_label[0])\n",
    "\n",
    "            # Delete the audio file\n",
    "            os.remove(output_audio_file)\n",
    "        else:\n",
    "            print(\"Error occurred while extracting features from recorded audio.\")\n",
    "\n",
    "        time.sleep(0.1)\n",
    "\n",
    "# Entry point\n",
    "if __name__ == \"__main__\":\n",
    "    record_and_classify_audio()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daee24ca",
   "metadata": {},
   "source": [
    "## slide window by 0.5 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c25622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "import time\n",
    "import os\n",
    "import pandas as pd  \n",
    "import torch\n",
    "\n",
    "\n",
    "def extract_features(file_name):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast')\n",
    "\n",
    "        # MFCC (Mel-frequency cepstral coefficients)\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "        mfccs_processed = np.mean(mfccs.T, axis=0)\n",
    "\n",
    "        # Chroma feature\n",
    "        chroma_stft = np.mean(librosa.feature.chroma_stft(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Spectral contrast\n",
    "        spectral_contrast = np.mean(librosa.feature.spectral_contrast(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Spectral centroid\n",
    "        spectral_centroids = np.mean(librosa.feature.spectral_centroid(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Zero-crossing rate\n",
    "        zero_crossing_rate = np.mean(librosa.feature.zero_crossing_rate(y=audio).T, axis=0)\n",
    "\n",
    "        # Spectral rolloff\n",
    "        spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Combine all features into a 1D array\n",
    "        features = np.hstack([mfccs_processed, chroma_stft, spectral_contrast, spectral_centroids, zero_crossing_rate, spectral_rolloff])\n",
    "\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"Error encountered while parsing file: {file_name}\")\n",
    "        return None\n",
    "\n",
    "def record_and_classify_audio():\n",
    "    audio_duration = 60  # Record audio for 1 minute\n",
    "    sample_rate = 44100  # 44.1kHz\n",
    "    _frames_ = int(sample_rate / 1024 * 0.5)  \n",
    "\n",
    "    audio_frames = []\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    stream = p.open(format=pyaudio.paInt16, channels=1, rate=sample_rate, input=True, frames_per_buffer=1024)\n",
    "    \n",
    "    # Initialize the buffer with 1-minute audio data\n",
    "    print(\"Initializing audio buffer with 1-minute audio...\")\n",
    "    for _ in range(0, int(sample_rate / 1024 * audio_duration)):\n",
    "        data = stream.read(1024)\n",
    "        audio_frames.append(data)\n",
    "    print(\"Initialization complete.\")\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        print(\"Sliding the window by 0.5 seconds...\")\n",
    "        audio_frames = audio_frames[_frames_:]\n",
    "        \n",
    "        for _ in range(0, _frames_):\n",
    "            data = stream.read(1024)\n",
    "            audio_frames.append(data)\n",
    "\n",
    "        \n",
    "        output_audio_file = \"recorded_audio.wav\"\n",
    "        wf = wave.open(output_audio_file, 'wb')\n",
    "        wf.setnchannels(1)\n",
    "        wf.setsampwidth(p.get_sample_size(pyaudio.paInt16))\n",
    "        wf.setframerate(sample_rate)\n",
    "        wf.writeframes(b''.join(audio_frames))\n",
    "        wf.close()\n",
    "\n",
    "        # extract features \n",
    "        recorded_features = extract_features(output_audio_file)\n",
    "\n",
    "        if recorded_features is not None:\n",
    "            recorded_df = pd.DataFrame([recorded_features])\n",
    "            recorded_df = scaler.transform(recorded_df)\n",
    "            model.eval()\n",
    "            recorded_tensor = torch.tensor(recorded_df, dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                predicted_output = model(recorded_tensor.unsqueeze(1))\n",
    "                _, predicted_class = torch.max(predicted_output, 1)\n",
    "\n",
    "            predicted_class_label = label_encoder.inverse_transform(predicted_class.numpy())\n",
    "            print(\"Predicted Class:\", predicted_class_label[0])\n",
    "\n",
    "            # Delete the audio file\n",
    "            os.remove(output_audio_file)\n",
    "        else:\n",
    "            print(\"Error occurred while extracting features from recorded audio.\")\n",
    "\n",
    "        time.sleep(0.1)\n",
    "\n",
    "# Entry point\n",
    "if __name__ == \"__main__\":\n",
    "    record_and_classify_audio()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87738cc",
   "metadata": {},
   "source": [
    "## slide window by 0.6 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba615fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "import time\n",
    "import os\n",
    "import pandas as pd  \n",
    "import torch\n",
    "\n",
    "\n",
    "def extract_features(file_name):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast')\n",
    "\n",
    "        # MFCC (Mel-frequency cepstral coefficients)\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "        mfccs_processed = np.mean(mfccs.T, axis=0)\n",
    "\n",
    "        # Chroma feature\n",
    "        chroma_stft = np.mean(librosa.feature.chroma_stft(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Spectral contrast\n",
    "        spectral_contrast = np.mean(librosa.feature.spectral_contrast(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Spectral centroid\n",
    "        spectral_centroids = np.mean(librosa.feature.spectral_centroid(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Zero-crossing rate\n",
    "        zero_crossing_rate = np.mean(librosa.feature.zero_crossing_rate(y=audio).T, axis=0)\n",
    "\n",
    "        # Spectral rolloff\n",
    "        spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Combine all features into a 1D array\n",
    "        features = np.hstack([mfccs_processed, chroma_stft, spectral_contrast, spectral_centroids, zero_crossing_rate, spectral_rolloff])\n",
    "\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"Error encountered while parsing file: {file_name}\")\n",
    "        return None\n",
    "\n",
    "def record_and_classify_audio():\n",
    "    audio_duration = 60  # Record audio for 1 minute\n",
    "    sample_rate = 44100  # 44.1kHz\n",
    "    _frames_ = int(sample_rate / 1024 * 0.6)  \n",
    "\n",
    "    audio_frames = []\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    stream = p.open(format=pyaudio.paInt16, channels=1, rate=sample_rate, input=True, frames_per_buffer=1024)\n",
    "    \n",
    "    # Initialize the buffer with 1-minute audio data\n",
    "    print(\"Initializing audio buffer with 1-minute audio...\")\n",
    "    for _ in range(0, int(sample_rate / 1024 * audio_duration)):\n",
    "        data = stream.read(1024)\n",
    "        audio_frames.append(data)\n",
    "    print(\"Initialization complete.\")\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        print(\"Sliding the window by 0.6 seconds...\")\n",
    "        audio_frames = audio_frames[_frames_:]\n",
    "        \n",
    "        for _ in range(0, _frames_):\n",
    "            data = stream.read(1024)\n",
    "            audio_frames.append(data)\n",
    "\n",
    "        \n",
    "        output_audio_file = \"recorded_audio.wav\"\n",
    "        wf = wave.open(output_audio_file, 'wb')\n",
    "        wf.setnchannels(1)\n",
    "        wf.setsampwidth(p.get_sample_size(pyaudio.paInt16))\n",
    "        wf.setframerate(sample_rate)\n",
    "        wf.writeframes(b''.join(audio_frames))\n",
    "        wf.close()\n",
    "\n",
    "        # extract features \n",
    "        recorded_features = extract_features(output_audio_file)\n",
    "\n",
    "        if recorded_features is not None:\n",
    "            recorded_df = pd.DataFrame([recorded_features])\n",
    "            recorded_df = scaler.transform(recorded_df)\n",
    "            model.eval()\n",
    "            recorded_tensor = torch.tensor(recorded_df, dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                predicted_output = model(recorded_tensor.unsqueeze(1))\n",
    "                _, predicted_class = torch.max(predicted_output, 1)\n",
    "\n",
    "            predicted_class_label = label_encoder.inverse_transform(predicted_class.numpy())\n",
    "            print(\"Predicted Class:\", predicted_class_label[0])\n",
    "\n",
    "            # Delete the audio file\n",
    "            os.remove(output_audio_file)\n",
    "        else:\n",
    "            print(\"Error occurred while extracting features from recorded audio.\")\n",
    "\n",
    "        time.sleep(0.1)\n",
    "\n",
    "# Entry point\n",
    "if __name__ == \"__main__\":\n",
    "    record_and_classify_audio()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b91268",
   "metadata": {},
   "source": [
    "## Sliding the window by 0.7 second\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a7fe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "import time\n",
    "import os\n",
    "import pandas as pd  \n",
    "import torch\n",
    "\n",
    "\n",
    "def extract_features(file_name):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast')\n",
    "\n",
    "        # MFCC (Mel-frequency cepstral coefficients)\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "        mfccs_processed = np.mean(mfccs.T, axis=0)\n",
    "\n",
    "        # Chroma feature\n",
    "        chroma_stft = np.mean(librosa.feature.chroma_stft(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Spectral contrast\n",
    "        spectral_contrast = np.mean(librosa.feature.spectral_contrast(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Spectral centroid\n",
    "        spectral_centroids = np.mean(librosa.feature.spectral_centroid(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Zero-crossing rate\n",
    "        zero_crossing_rate = np.mean(librosa.feature.zero_crossing_rate(y=audio).T, axis=0)\n",
    "\n",
    "        # Spectral rolloff\n",
    "        spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Combine all features into a 1D array\n",
    "        features = np.hstack([mfccs_processed, chroma_stft, spectral_contrast, spectral_centroids, zero_crossing_rate, spectral_rolloff])\n",
    "\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"Error encountered while parsing file: {file_name}\")\n",
    "        return None\n",
    "\n",
    "def record_and_classify_audio():\n",
    "    audio_duration = 60  # Record audio for 1 minute\n",
    "    sample_rate = 44100  # 44.1kHz\n",
    "    _frames_ = int(sample_rate / 1024 * 0.7)  \n",
    "\n",
    "    audio_frames = []\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    stream = p.open(format=pyaudio.paInt16, channels=1, rate=sample_rate, input=True, frames_per_buffer=1024)\n",
    "    \n",
    "    # Initialize the buffer with 1-minute audio data\n",
    "    print(\"Initializing audio buffer with 1-minute audio...\")\n",
    "    for _ in range(0, int(sample_rate / 1024 * audio_duration)):\n",
    "        data = stream.read(1024)\n",
    "        audio_frames.append(data)\n",
    "    print(\"Initialization complete.\")\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        print(\"Sliding the window by 0.7 seconds...\")\n",
    "        audio_frames = audio_frames[_frames_:]\n",
    "        \n",
    "        for _ in range(0, _frames_):\n",
    "            data = stream.read(1024)\n",
    "            audio_frames.append(data)\n",
    "\n",
    "        \n",
    "        output_audio_file = \"recorded_audio.wav\"\n",
    "        wf = wave.open(output_audio_file, 'wb')\n",
    "        wf.setnchannels(1)\n",
    "        wf.setsampwidth(p.get_sample_size(pyaudio.paInt16))\n",
    "        wf.setframerate(sample_rate)\n",
    "        wf.writeframes(b''.join(audio_frames))\n",
    "        wf.close()\n",
    "\n",
    "        # extract features \n",
    "        recorded_features = extract_features(output_audio_file)\n",
    "\n",
    "        if recorded_features is not None:\n",
    "            recorded_df = pd.DataFrame([recorded_features])\n",
    "            recorded_df = scaler.transform(recorded_df)\n",
    "            model.eval()\n",
    "            recorded_tensor = torch.tensor(recorded_df, dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                predicted_output = model(recorded_tensor.unsqueeze(1))\n",
    "                _, predicted_class = torch.max(predicted_output, 1)\n",
    "\n",
    "            predicted_class_label = label_encoder.inverse_transform(predicted_class.numpy())\n",
    "            print(\"Predicted Class:\", predicted_class_label[0])\n",
    "\n",
    "            # Delete the audio file\n",
    "            os.remove(output_audio_file)\n",
    "        else:\n",
    "            print(\"Error occurred while extracting features from recorded audio.\")\n",
    "\n",
    "        time.sleep(0.1)\n",
    "\n",
    "# Entry point\n",
    "if __name__ == \"__main__\":\n",
    "    record_and_classify_audio()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3509bb",
   "metadata": {},
   "source": [
    "## Sliding the window by 0.8 second\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cf6f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "import time\n",
    "import os\n",
    "import pandas as pd  \n",
    "import torch\n",
    "\n",
    "\n",
    "def extract_features(file_name):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast')\n",
    "\n",
    "        # MFCC (Mel-frequency cepstral coefficients)\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "        mfccs_processed = np.mean(mfccs.T, axis=0)\n",
    "\n",
    "        # Chroma feature\n",
    "        chroma_stft = np.mean(librosa.feature.chroma_stft(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Spectral contrast\n",
    "        spectral_contrast = np.mean(librosa.feature.spectral_contrast(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Spectral centroid\n",
    "        spectral_centroids = np.mean(librosa.feature.spectral_centroid(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Zero-crossing rate\n",
    "        zero_crossing_rate = np.mean(librosa.feature.zero_crossing_rate(y=audio).T, axis=0)\n",
    "\n",
    "        # Spectral rolloff\n",
    "        spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Combine all features into a 1D array\n",
    "        features = np.hstack([mfccs_processed, chroma_stft, spectral_contrast, spectral_centroids, zero_crossing_rate, spectral_rolloff])\n",
    "\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"Error encountered while parsing file: {file_name}\")\n",
    "        return None\n",
    "\n",
    "def record_and_classify_audio():\n",
    "    audio_duration = 60  # Record audio for 1 minute\n",
    "    sample_rate = 44100  # 44.1kHz\n",
    "    _frames_ = int(sample_rate / 1024 * 0.8)  \n",
    "\n",
    "    audio_frames = []\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    stream = p.open(format=pyaudio.paInt16, channels=1, rate=sample_rate, input=True, frames_per_buffer=1024)\n",
    "    \n",
    "    # Initialize the buffer with 1-minute audio data\n",
    "    print(\"Initializing audio buffer with 1-minute audio...\")\n",
    "    for _ in range(0, int(sample_rate / 1024 * audio_duration)):\n",
    "        data = stream.read(1024)\n",
    "        audio_frames.append(data)\n",
    "    print(\"Initialization complete.\")\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        print(\"Sliding the window by 0.8 seconds...\")\n",
    "        audio_frames = audio_frames[_frames_:]\n",
    "        \n",
    "        for _ in range(0, _frames_):\n",
    "            data = stream.read(1024)\n",
    "            audio_frames.append(data)\n",
    "\n",
    "        \n",
    "        output_audio_file = \"recorded_audio.wav\"\n",
    "        wf = wave.open(output_audio_file, 'wb')\n",
    "        wf.setnchannels(1)\n",
    "        wf.setsampwidth(p.get_sample_size(pyaudio.paInt16))\n",
    "        wf.setframerate(sample_rate)\n",
    "        wf.writeframes(b''.join(audio_frames))\n",
    "        wf.close()\n",
    "\n",
    "        # extract features \n",
    "        recorded_features = extract_features(output_audio_file)\n",
    "\n",
    "        if recorded_features is not None:\n",
    "            recorded_df = pd.DataFrame([recorded_features])\n",
    "            recorded_df = scaler.transform(recorded_df)\n",
    "            model.eval()\n",
    "            recorded_tensor = torch.tensor(recorded_df, dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                predicted_output = model(recorded_tensor.unsqueeze(1))\n",
    "                _, predicted_class = torch.max(predicted_output, 1)\n",
    "\n",
    "            predicted_class_label = label_encoder.inverse_transform(predicted_class.numpy())\n",
    "            print(\"Predicted Class:\", predicted_class_label[0])\n",
    "\n",
    "            # Delete the audio file\n",
    "            os.remove(output_audio_file)\n",
    "        else:\n",
    "            print(\"Error occurred while extracting features from recorded audio.\")\n",
    "\n",
    "        time.sleep(0.1)\n",
    "\n",
    "# Entry point\n",
    "if __name__ == \"__main__\":\n",
    "    record_and_classify_audio()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9863f0c",
   "metadata": {},
   "source": [
    "## Sliding the window by 0.9 second\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dd38ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "import time\n",
    "import os\n",
    "import pandas as pd  \n",
    "import torch\n",
    "\n",
    "\n",
    "def extract_features(file_name):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast')\n",
    "\n",
    "        # MFCC (Mel-frequency cepstral coefficients)\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "        mfccs_processed = np.mean(mfccs.T, axis=0)\n",
    "\n",
    "        # Chroma feature\n",
    "        chroma_stft = np.mean(librosa.feature.chroma_stft(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Spectral contrast\n",
    "        spectral_contrast = np.mean(librosa.feature.spectral_contrast(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Spectral centroid\n",
    "        spectral_centroids = np.mean(librosa.feature.spectral_centroid(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Zero-crossing rate\n",
    "        zero_crossing_rate = np.mean(librosa.feature.zero_crossing_rate(y=audio).T, axis=0)\n",
    "\n",
    "        # Spectral rolloff\n",
    "        spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Combine all features into a 1D array\n",
    "        features = np.hstack([mfccs_processed, chroma_stft, spectral_contrast, spectral_centroids, zero_crossing_rate, spectral_rolloff])\n",
    "\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"Error encountered while parsing file: {file_name}\")\n",
    "        return None\n",
    "\n",
    "def record_and_classify_audio():\n",
    "    audio_duration = 60  # Record audio for 1 minute\n",
    "    sample_rate = 44100  # 44.1kHz\n",
    "    _frames_ = int(sample_rate / 1024 * 0.9)  \n",
    "\n",
    "    audio_frames = []\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    stream = p.open(format=pyaudio.paInt16, channels=1, rate=sample_rate, input=True, frames_per_buffer=1024)\n",
    "    \n",
    "    # Initialize the buffer with 1-minute audio data\n",
    "    print(\"Initializing audio buffer with 1-minute audio...\")\n",
    "    for _ in range(0, int(sample_rate / 1024 * audio_duration)):\n",
    "        data = stream.read(1024)\n",
    "        audio_frames.append(data)\n",
    "    print(\"Initialization complete.\")\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        print(\"Sliding the window by 0.9 seconds...\")\n",
    "        audio_frames = audio_frames[_frames_:]\n",
    "        \n",
    "        for _ in range(0, _frames_):\n",
    "            data = stream.read(1024)\n",
    "            audio_frames.append(data)\n",
    "\n",
    "        \n",
    "        output_audio_file = \"recorded_audio.wav\"\n",
    "        wf = wave.open(output_audio_file, 'wb')\n",
    "        wf.setnchannels(1)\n",
    "        wf.setsampwidth(p.get_sample_size(pyaudio.paInt16))\n",
    "        wf.setframerate(sample_rate)\n",
    "        wf.writeframes(b''.join(audio_frames))\n",
    "        wf.close()\n",
    "\n",
    "        # extract features \n",
    "        recorded_features = extract_features(output_audio_file)\n",
    "\n",
    "        if recorded_features is not None:\n",
    "            recorded_df = pd.DataFrame([recorded_features])\n",
    "            recorded_df = scaler.transform(recorded_df)\n",
    "            model.eval()\n",
    "            recorded_tensor = torch.tensor(recorded_df, dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                predicted_output = model(recorded_tensor.unsqueeze(1))\n",
    "                _, predicted_class = torch.max(predicted_output, 1)\n",
    "\n",
    "            predicted_class_label = label_encoder.inverse_transform(predicted_class.numpy())\n",
    "            print(\"Predicted Class:\", predicted_class_label[0])\n",
    "\n",
    "            # Delete the audio file\n",
    "            os.remove(output_audio_file)\n",
    "        else:\n",
    "            print(\"Error occurred while extracting features from recorded audio.\")\n",
    "\n",
    "        time.sleep(0.1)\n",
    "\n",
    "# Entry point\n",
    "if __name__ == \"__main__\":\n",
    "    record_and_classify_audio()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990804a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8cc9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def test_video_capture():\n",
    "    cap = cv2.VideoCapture(1)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Couldn't open the camera.\")\n",
    "        return\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Couldn't read a frame.\")\n",
    "            break\n",
    "\n",
    "        cv2.imshow('Test', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Call the function\n",
    "test_video_capture()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540b99ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3b6588",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3c34d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

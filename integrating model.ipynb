{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02d69306",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final one\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision import transforms, models \n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import wave\n",
    "import pyaudio\n",
    "import time\n",
    "from collections import Counter\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "features_df = pd.read_csv('features.csv')\n",
    "\n",
    "# Extract features and corresponding labels\n",
    "X = features_df.iloc[:, :-1].values  # features \n",
    "y = features_df.iloc[:, -1].values  # labels \n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "# Encode the labels\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.int64)\n",
    "\n",
    "# Split the dataset into training, validation, and testing sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42d79a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/waqar/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/waqar/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CustomVGG16(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): ReLU(inplace=True)\n",
       "    (32): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.7, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.7, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import threading\n",
    "import pyaudio\n",
    "import time\n",
    "import wave\n",
    "import torch\n",
    "import numpy as np\n",
    "import librosa\n",
    "from collections import deque, Counter\n",
    "from torchvision import transforms, models\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from PIL import Image  \n",
    "\n",
    "\n",
    "# Constants\n",
    "FRAME_SKIP = 3\n",
    "FRAME_PROCESS_INTERVAL = 10\n",
    "SAMPLE_RATE = 30\n",
    "input_size = 62\n",
    "hidden_size = 64\n",
    "num_classes = 2\n",
    "sample_rate = 44100\n",
    "\n",
    "# Image transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "class CustomVGG16(nn.Module):\n",
    "    def __init__(self, num_classes=6, dropout_rate=0.7):\n",
    "        super(CustomVGG16, self).__init__()\n",
    "\n",
    "        model = models.vgg16(pretrained=True)\n",
    "        for param in model.features[8:].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            *list(model.features.children())[:24],\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            *list(model.features.children())[24:],\n",
    "        )\n",
    "        self.avgpool = model.avgpool\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "# Class names\n",
    "class_names = ['Automotive_commercial', 'Entertainment_commercial', 'Food_commercial',\n",
    "               'Healthcare_commercial', 'Insurance_commercial', 'Technology_Electronics_commercial']\n",
    "\n",
    "# Initialize and load the VGG16 model\n",
    "model = CustomVGG16(num_classes=6)\n",
    "model.load_state_dict(torch.load('best_model_checkpoint.pth', map_location='cpu'))\n",
    "model.to('cpu')\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59553934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (rnn): RNN(62, 64, batch_first=True)\n",
       "  (fc): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the RNN model\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "input_size = 62  \n",
    "hidden_size = 64\n",
    "num_classes = 2 \n",
    "\n",
    "model2 = RNN(input_size, hidden_size, num_classes)\n",
    "model2.load_state_dict(torch.load('rnn_model.pth', map_location='cpu'))\n",
    "model2.to('cpu')\n",
    "model2.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90c454bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file_name):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast')\n",
    "\n",
    "        # MFCC (Mel-frequency cepstral coefficients)\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "        mfccs_processed = np.mean(mfccs.T, axis=0)\n",
    "\n",
    "        # Chroma feature\n",
    "        chroma_stft = np.mean(librosa.feature.chroma_stft(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Spectral contrast\n",
    "        spectral_contrast = np.mean(librosa.feature.spectral_contrast(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Spectral centroid\n",
    "        spectral_centroids = np.mean(librosa.feature.spectral_centroid(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Zero-crossing rate\n",
    "        zero_crossing_rate = np.mean(librosa.feature.zero_crossing_rate(y=audio).T, axis=0)\n",
    "\n",
    "        # Spectral rolloff\n",
    "        spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=audio, sr=sample_rate).T, axis=0)\n",
    "\n",
    "        # Combine all features into a 1D array\n",
    "        features = np.hstack([mfccs_processed, chroma_stft, spectral_contrast, spectral_centroids, zero_crossing_rate, spectral_rolloff])\n",
    "\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"Error encountered while parsing file: {file_name}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e77fce39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slide_and_record_additional_audio(existing_frames, seconds_to_record=10):\n",
    "    sample_rate = 44100\n",
    "    frames_to_discard = int(sample_rate / 1024 * seconds_to_record)\n",
    "\n",
    "    # Discard the first 'seconds_to_record' worth of frames\n",
    "    existing_frames = existing_frames[frames_to_discard:]\n",
    "\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format=pyaudio.paInt16, channels=1, rate=sample_rate, input=True, frames_per_buffer=1024)\n",
    "\n",
    "    # Record new audio to append\n",
    "    for _ in range(0, frames_to_discard):\n",
    "        data = stream.read(1024)\n",
    "        existing_frames.append(data)\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "    return existing_frames  # Return the updated frames\n",
    "\n",
    "\n",
    "def classify_frames_using_vgg(frame):\n",
    "    orig_frame = frame.copy()\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    pil_image = Image.fromarray(frame)\n",
    "    transformed_image = transform(pil_image)\n",
    "    batch_t = torch.unsqueeze(transformed_image, 0).to('cpu')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(batch_t)\n",
    "\n",
    "    _, predicted = torch.max(out, 1)\n",
    "    label = class_names[predicted.item()]\n",
    "    cv2.putText(orig_frame, label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "    \n",
    "    return orig_frame \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d54d6033",
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAME_PROCESS_DURATION = 5 \n",
    "\n",
    "def record_and_classify_audio(video_feed):\n",
    "    audio_duration = 60  # seconds\n",
    "    sample_rate = 44100\n",
    "    audio_frames = []\n",
    "\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format=pyaudio.paInt16, channels=1, rate=sample_rate, input=True, frames_per_buffer=1024)\n",
    "\n",
    "    global commercial_detected\n",
    "    commercial_detected = False \n",
    "\n",
    "    \n",
    "    print(\"Starting audio recording and classification...\")\n",
    "    while True:\n",
    "        # Record audio for the specified duration\n",
    "        if len(audio_frames) < int(sample_rate / 1024 * audio_duration):\n",
    "            data = stream.read(1024)\n",
    "            audio_frames.append(data)\n",
    "        else:\n",
    "            # Save the recorded audio to a WAV file\n",
    "            output_audio_file = \"recorded_audio.wav\"\n",
    "            with wave.open(output_audio_file, 'wb') as wf:\n",
    "                wf.setnchannels(1)\n",
    "                wf.setsampwidth(p.get_sample_size(pyaudio.paInt16))\n",
    "                wf.setframerate(sample_rate)\n",
    "                wf.writeframes(b''.join(audio_frames))\n",
    "\n",
    "            # Extract features from the audio file\n",
    "            recorded_features = extract_features(output_audio_file)\n",
    "            if recorded_features is not None:\n",
    "                recorded_df = pd.DataFrame([recorded_features])\n",
    "                scaler = StandardScaler().fit(recorded_df)\n",
    "                recorded_df = scaler.transform(recorded_df)\n",
    "\n",
    "                model2.eval()\n",
    "                recorded_tensor = torch.tensor(recorded_df, dtype=torch.float32)\n",
    "                with torch.no_grad():\n",
    "\n",
    "                    predicted_output = torch.sigmoid(model2(recorded_tensor.unsqueeze(1))[:, -1])\n",
    "                    \n",
    "                    predicted_class = (predicted_output).float()\n",
    "                # Run VGG for 10 seconds if class is 0.0\n",
    "                if predicted_class.item() == 0.0:\n",
    "                    print(\"Audio classified as commercial. Starting VGG16 model for 10 seconds.\")\n",
    "                    print(predicted_class.item())\n",
    "                    start_time = time.time()  # Save the current time\n",
    "                    commercial_detected = True  # Set flag to true\n",
    "                else:\n",
    "                    print(\"Audio classified as non-commercial. Stopping VGG16 model.\")\n",
    "                    commercial_detected = False  # Set flag to false\n",
    "\n",
    "\n",
    "            # Slide audio window\n",
    "            audio_frames = slide_and_record_additional_audio(audio_frames)\n",
    "\n",
    "        ret, frame = video_feed.read()\n",
    "        if ret:\n",
    "            # Check if a commercial is detected\n",
    "            if commercial_detected:\n",
    "                frame = classify_frames_using_vgg(frame)  # This function returns a modified frame with label\n",
    "\n",
    "                # Check if we have processed the frames for more than 5 seconds\n",
    "                if time.time() - start_time > FRAME_PROCESS_DURATION:\n",
    "                    commercial_detected = False\n",
    "                    \n",
    "            # Check if the frame is valid before displaying\n",
    "            if frame is not None and isinstance(frame, np.ndarray):\n",
    "                cv2.imshow('Live Feed with Classifications', frame)\n",
    "            \n",
    "            # Exit loop when 'q' is pressed\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945aad3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@5.497] global cap_v4l.cpp:982 open VIDEOIO(V4L2:/dev/video0): can't open camera by index\n",
      "[ERROR:0@5.497] global obsensor_uvc_stream_channel.cpp:156 getStreamChannelGroup Camera index out of range\n",
      "ALSA lib pcm_dsnoop.c:601:(snd_pcm_dsnoop_open) unable to open slave\n",
      "ALSA lib pcm_dmix.c:1032:(snd_pcm_dmix_open) unable to open slave\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.rear\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.center_lfe\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.side\n",
      "ALSA lib pcm_oss.c:397:(_snd_pcm_oss_open) Cannot open device /dev/dsp\n",
      "ALSA lib pcm_oss.c:397:(_snd_pcm_oss_open) Cannot open device /dev/dsp\n",
      "ALSA lib confmisc.c:160:(snd_config_get_card) Invalid field card\n",
      "ALSA lib pcm_usb_stream.c:482:(_snd_pcm_usb_stream_open) Invalid card 'card'\n",
      "ALSA lib confmisc.c:160:(snd_config_get_card) Invalid field card\n",
      "ALSA lib pcm_usb_stream.c:482:(_snd_pcm_usb_stream_open) Invalid card 'card'\n",
      "ALSA lib pcm_dmix.c:1032:(snd_pcm_dmix_open) unable to open slave\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Could not open video capture.\n",
      "Starting audio recording and classification...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/waqar/.local/lib/python3.10/site-packages/librosa/util/decorators.py:88: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio classified as non-commercial. Stopping VGG16 model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/waqar/.local/lib/python3.10/site-packages/librosa/util/decorators.py:88: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio classified as non-commercial. Stopping VGG16 model.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Thread to display and process frames\n",
    "def display_and_process_frames():\n",
    "    global commercial_detected  # Include this to consider RNN model output\n",
    "\n",
    "    cap = cv2.VideoCapture(2)\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        with lock:\n",
    "            if commercial_detected:  # Check this flag here\n",
    "                frame = classify_frames_using_vgg(frame)\n",
    "\n",
    "        \n",
    "        cv2.imshow('Live Feed with Classifications', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Main Function\n",
    "if __name__ == \"__main__\":\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video capture.\")\n",
    "        exit()\n",
    "\n",
    "    audio_thread = threading.Thread(target=record_and_classify_audio, args=(cap,))\n",
    "    video_thread = threading.Thread(target=display_and_process_frames)\n",
    "    \n",
    "    audio_thread.start()\n",
    "    video_thread.start()\n",
    "    \n",
    "    audio_thread.join()\n",
    "    video_thread.join()\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ab8057",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed27e88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a20cd27",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mp3\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     47\u001b[0m     audio_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(commercial_path, file)\n\u001b[0;32m---> 48\u001b[0m     fingerprint \u001b[38;5;241m=\u001b[39m \u001b[43mextract_fingerprints\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     fingerprints\u001b[38;5;241m.\u001b[39mappend(fingerprint)\n\u001b[1;32m     50\u001b[0m     labels\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# 1 represents commercial audio\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 25\u001b[0m, in \u001b[0;36mextract_fingerprints\u001b[0;34m(audio_path)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_fingerprints\u001b[39m(audio_path):\n\u001b[0;32m---> 25\u001b[0m     audio, sr \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Load the audio file\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# Extract MFCC features\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     mfcc \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mmfcc(audio, sr\u001b[38;5;241m=\u001b[39msr, n_mfcc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m13\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/librosa/core/audio.py:176\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# Otherwise try soundfile first, and then fall back if necessary\u001b[39;00m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m         y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__soundfile_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m sf\u001b[38;5;241m.\u001b[39mSoundFileRuntimeError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;66;03m# If soundfile failed, try audioread instead\u001b[39;00m\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, (\u001b[38;5;28mstr\u001b[39m, pathlib\u001b[38;5;241m.\u001b[39mPurePath)):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/librosa/core/audio.py:222\u001b[0m, in \u001b[0;36m__soundfile_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[1;32m    219\u001b[0m         frame_duration \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# Load the target number of frames, and transpose to match librosa form\u001b[39;00m\n\u001b[0;32m--> 222\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43msf_desc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_duration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malways_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y, sr_native\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/soundfile.py:895\u001b[0m, in \u001b[0;36mSoundFile.read\u001b[0;34m(self, frames, dtype, always_2d, fill_value, out)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m frames \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m frames \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlen\u001b[39m(out):\n\u001b[1;32m    894\u001b[0m         frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(out)\n\u001b[0;32m--> 895\u001b[0m frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_array_io\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mread\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m>\u001b[39m frames:\n\u001b[1;32m    897\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/soundfile.py:1344\u001b[0m, in \u001b[0;36mSoundFile._array_io\u001b[0;34m(self, action, array, frames)\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mitemsize \u001b[38;5;241m==\u001b[39m _ffi\u001b[38;5;241m.\u001b[39msizeof(ctype)\n\u001b[1;32m   1343\u001b[0m cdata \u001b[38;5;241m=\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mcast(ctype \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m, array\u001b[38;5;241m.\u001b[39m__array_interface__[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m-> 1344\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cdata_io\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/soundfile.py:1353\u001b[0m, in \u001b[0;36mSoundFile._cdata_io\u001b[0;34m(self, action, data, ctype, frames)\u001b[0m\n\u001b[1;32m   1351\u001b[0m     curr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtell()\n\u001b[1;32m   1352\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_snd, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msf_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m action \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m ctype)\n\u001b[0;32m-> 1353\u001b[0m frames \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1354\u001b[0m _error_check(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_errorcode)\n\u001b[1;32m   1355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseekable():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Define the paths to the audio files\n",
    "commercial_path = 'audio_directory/commercial'\n",
    "non_commercial_path = 'audio_directory/non_commercial'\n",
    "\n",
    "# Prepare empty lists for features and labels\n",
    "fingerprints = []\n",
    "labels = []\n",
    "\n",
    "def extract_fingerprints(audio_path):\n",
    "    audio, sr = librosa.load(audio_path)  # Load the audio file\n",
    "\n",
    "    # Extract MFCC features\n",
    "    mfcc = librosa.feature.mfcc(audio, sr=sr, n_mfcc=13)\n",
    "\n",
    "    # Extract Chroma features\n",
    "    chroma = librosa.feature.chroma_stft(audio, sr=sr)\n",
    "\n",
    "    # Extract Spectral Contrast features\n",
    "    contrast = librosa.feature.spectral_contrast(audio, sr=sr)\n",
    "\n",
    "    # Extract Tonnetz features\n",
    "    tonnetz = librosa.feature.tonnetz(audio, sr=sr)\n",
    "\n",
    "    # Stack all features along the channel dimension\n",
    "    fingerprint = np.stack((mfcc, chroma, contrast, tonnetz), axis=0)\n",
    "    return fingerprint\n",
    "\n",
    "# Read and extract fingerprints for commercial audio files\n",
    "commercial_files = os.listdir(commercial_path)\n",
    "for file in commercial_files:\n",
    "    if file.endswith('.mp3'):\n",
    "        audio_path = os.path.join(commercial_path, file)\n",
    "        fingerprint = extract_fingerprints(audio_path)\n",
    "        fingerprints.append(fingerprint)\n",
    "        labels.append(1)  # 1 represents commercial audio\n",
    "\n",
    "# Read and extract fingerprints for non-commercial audio files\n",
    "non_commercial_files = os.listdir(non_commercial_path)\n",
    "for file in non_commercial_files:\n",
    "    if file.endswith('.mp3'):\n",
    "        audio_path = os.path.join(non_commercial_path, file)\n",
    "        fingerprint = extract_fingerprints(audio_path)\n",
    "        fingerprints.append(fingerprint)\n",
    "        labels.append(0)  # 0 represents non-commercial audio\n",
    "\n",
    "# Convert the lists to numpy arrays\n",
    "fingerprints = np.array(fingerprints)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Preprocessing data\n",
    "scaler = StandardScaler()\n",
    "fingerprints = fingerprints.reshape(fingerprints.shape[0], -1)  # Flatten the fingerprints\n",
    "fingerprints = scaler.fit_transform(fingerprints)\n",
    "fingerprints = fingerprints.reshape(fingerprints.shape[0], 4, -1)  # Reshape the flattened fingerprints\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(fingerprints, labels, test_size=0.2, random_state=seed, shuffle=True)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train = torch.Tensor(X_train)\n",
    "X_test = torch.Tensor(X_test)\n",
    "y_train = torch.Tensor(y_train)\n",
    "y_test = torch.Tensor(y_test)\n",
    "\n",
    "# Create a DataLoader for training\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# Define the model architecture\n",
    "model = nn.Sequential(\n",
    "    nn.Conv1d(4, 32, kernel_size=3),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool1d(kernel_size=2),\n",
    "    nn.Conv1d(32, 64, kernel_size=3),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool1d(kernel_size=2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(64 * 6, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 1),\n",
    "    nn.Sigmoid()  # Use sigmoid activation for binary classification\n",
    ")\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs.squeeze(), target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, running_loss/len(train_loader)))\n",
    "\n",
    "# Evaluation on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test)\n",
    "    test_predicted = (test_outputs > 0.5).float()\n",
    "    test_correct = (test_predicted == y_test).sum().item()\n",
    "    test_total = y_test.size(0)\n",
    "    test_accuracy = test_correct / test_total\n",
    "    print('Test Accuracy: {:.2%}'.format(test_accuracy))\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347f0148",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
